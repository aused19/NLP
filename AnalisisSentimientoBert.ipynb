{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://www.ceste.es/wp-content/uploads/2022/03/logotipo-ceste-escuela-internacional-de-negocios.svg\" width=\"500\"/>\n",
        "\n",
        "# Procesamiento de lenguaje natural\n",
        "\n",
        "## Analisis Sentimiento Imdb con Bert (embeddings y modelo)\n",
        "\n",
        "Pablo Gómez Guerrero"
      ],
      "metadata": {
        "id": "wqzK9MyrzsTn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Análisis de sentimiento\n",
        "\n",
        "En esta ocasión trataremos el mismo reto que vimos anteriormente sobre la base de datos de opioniones de IMDB, con la intención de mejorar las métricas que teníamos (*accuracy 0.87 aprox*)\n",
        "\n",
        "Solo por recordar, en el [reto](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) tenemos 50k opiniones de películas etiquetadas con un sentimiento positivo o negativo."
      ],
      "metadata": {
        "id": "w6_nQc46z_Jp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT\n",
        "\n",
        "En este notebook usaremos BERT (Bidirectional Encoder Representations from Transformers) que es un Transformer desarrollado por Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova en 2018 para Google. BERT está entrenado con unos 2500 Millones de palabras en inglés obtenidas de Wikipedia y 800 Millones de palabras de BookCorpus.\n",
        "\n",
        "BERT es famoso por su potente rendimiento para tareas de NLP siendo el estado del arte en 2018. Una de las tareas que mejor realiza es el análisis de sentimiento, como veremos a continuación haciendo lo que se llama un Fine-tuning."
      ],
      "metadata": {
        "id": "OtCq6y7-2GGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-11T21:57:14.269155Z",
          "iopub.execute_input": "2023-05-11T21:57:14.269512Z",
          "iopub.status.idle": "2023-05-11T21:57:14.274464Z",
          "shell.execute_reply.started": "2023-05-11T21:57:14.269482Z",
          "shell.execute_reply": "2023-05-11T21:57:14.273490Z"
        },
        "trusted": true,
        "id": "f7pCQKuI1pA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import sklearn\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "execution": {
          "iopub.status.busy": "2023-05-11T21:57:07.802622Z",
          "iopub.execute_input": "2023-05-11T21:57:07.803289Z",
          "iopub.status.idle": "2023-05-11T21:57:14.267829Z",
          "shell.execute_reply.started": "2023-05-11T21:57:07.803248Z",
          "shell.execute_reply": "2023-05-11T21:57:14.266893Z"
        },
        "trusted": true,
        "id": "fVBqpQUB1pA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zztIKmtg354P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/CESTE_2023/data/IMDB Dataset.csv\")\n",
        "df.sample()"
      ],
      "metadata": {
        "id": "PC90UKB43Us1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  clasificador y Tokenizer de BERT\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from transformers import InputExample, InputFeatures\n",
        "\n",
        "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-11T21:57:14.276141Z",
          "iopub.execute_input": "2023-05-11T21:57:14.276951Z",
          "iopub.status.idle": "2023-05-11T21:57:41.454971Z",
          "shell.execute_reply.started": "2023-05-11T21:57:14.276912Z",
          "shell.execute_reply": "2023-05-11T21:57:41.453984Z"
        },
        "trusted": true,
        "id": "CJGEWxcl1pA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-11T21:57:41.459115Z",
          "iopub.execute_input": "2023-05-11T21:57:41.459416Z",
          "iopub.status.idle": "2023-05-11T21:57:41.463384Z",
          "shell.execute_reply.started": "2023-05-11T21:57:41.459385Z",
          "shell.execute_reply": "2023-05-11T21:57:41.462245Z"
        },
        "trusted": true,
        "id": "72VWwOt61pA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vamos a cambiar la anotaciones\n",
        "\n",
        "def cat2num(value):\n",
        "    if value=='positive':\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-11T21:57:41.466124Z",
          "iopub.execute_input": "2023-05-11T21:57:41.466751Z",
          "iopub.status.idle": "2023-05-11T21:57:42.401878Z",
          "shell.execute_reply.started": "2023-05-11T21:57:41.466710Z",
          "shell.execute_reply": "2023-05-11T21:57:42.400886Z"
        },
        "trusted": true,
        "id": "eRtDYW4s1pA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividimos el dataset en train y test\n",
        "\n",
        "df['sentiment']  =  df['sentiment'].apply(cat2num)\n",
        "train = df[:45000]\n",
        "test = df[45000:]"
      ],
      "metadata": {
        "id": "EH9dMoaO5E5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Procesamiento de datos\n",
        "\n",
        "Para hacer el Fine-tuning con BERT, como en otras ocasiones, hay varios pasos que necesitamos dar previamente.\n",
        "\n",
        "\n",
        "1.   tokenizar y separar las frases para clasificar\n",
        "2.   Las secuencias tienen que tener una logitud igual - padding\n",
        "3.   Crear una capa que se llama mascara de atención (para el transformer) con un array de 0s (pad token) y uno de 1s (real token)\n",
        "\n"
      ],
      "metadata": {
        "id": "qgWTKVkP7aIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Veamos primero como es el tokenizador\n",
        "\n",
        "example='We will work on sentiment analysis using BERT in this notebook'\n",
        "tokens = tokenizer.tokenize(example)\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(tokens)\n",
        "print(token_ids)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-11T21:57:42.403375Z",
          "iopub.execute_input": "2023-05-11T21:57:42.403804Z",
          "iopub.status.idle": "2023-05-11T21:57:42.413344Z",
          "shell.execute_reply.started": "2023-05-11T21:57:42.403733Z",
          "shell.execute_reply": "2023-05-11T21:57:42.412149Z"
        },
        "trusted": true,
        "id": "Pdm_WkXd1pA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Particularidades de BERT\n",
        "\n",
        "Bert trabaja con un vocabulario fijo de ~30k palabras, divididos subpalabras y caracteres. Así pues, no todas las palabras del diccionario están representadas en el vocabulario, para ello se usan las subpalabras y caracteres.\n",
        "\n",
        "El tokenizador juega un papel fundamental pues decide si mantener las palabras o dividirlas en algo más pequeño.\n",
        "\n",
        "### Representaciones especiales\n",
        "\n",
        "Además BERT necesita de unas representaciones especiales para trabajar con el texto. Estas respresentaciones son marcas dentro del texto que ayudan al sistema.\n",
        "\n",
        "1.   [SEP] - marca el fin de frase\n",
        "2.   [CLS] - marca el inicio de frase\n",
        "3.   [PAD] - padding\n",
        "4.   [UNK] - tokens que BERT no entiende porque no están en el set de entrenamiento"
      ],
      "metadata": {
        "id": "N5MNQFbM9OQP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pasos para entrenar\n",
        "\n",
        "Ahora vamos a adaptar los dataset a la forma con la que trabaja BERT. Para ellos usaremos 2 pasos.\n",
        "\n",
        "1. Convertir los datos en muestras válidas\n",
        "2. Converitr las muestras en objetos tokenizados\n"
      ],
      "metadata": {
        "id": "n7NoqO_005KY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### convert_data_to_examples\n",
        "\n",
        "Esta función convertirá nuestros datsets de entrenamiento y validación en ejemplos de entrada para BERT. Usamos unas labda para recorrer todos las muestras.\n",
        "\n"
      ],
      "metadata": {
        "id": "pZDAhuzP1pA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_data_to_examples(train, test, review, sentiment):\n",
        "    train_InputExamples = train.apply(lambda x: InputExample(guid=None, # No definimos este parámetro pero si lo hiciéramos es como un marcador que nos puede servir para guardar esta muestra para el futuro\n",
        "                                                          text_a = x[review],\n",
        "                                                          label = x[sentiment]), axis = 1)\n",
        "\n",
        "    validation_InputExamples = test.apply(lambda x: InputExample(guid=None,\n",
        "                                                          text_a = x[review],\n",
        "                                                          label = x[sentiment]), axis = 1,)\n",
        "\n",
        "    return train_InputExamples, validation_InputExamples"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-11T21:57:42.415059Z",
          "iopub.execute_input": "2023-05-11T21:57:42.415957Z",
          "iopub.status.idle": "2023-05-11T21:57:43.621620Z",
          "shell.execute_reply.started": "2023-05-11T21:57:42.415918Z",
          "shell.execute_reply": "2023-05-11T21:57:43.620582Z"
        },
        "trusted": true,
        "id": "dmsi0YNP1pA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_InputExamples, validation_InputExamples = convert_data_to_examples(train,  test, 'review',  'sentiment')"
      ],
      "metadata": {
        "id": "dv2GTkvS5Q-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_InputExamples[0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-11T21:57:43.626319Z",
          "iopub.execute_input": "2023-05-11T21:57:43.628520Z",
          "iopub.status.idle": "2023-05-11T21:57:43.636926Z",
          "shell.execute_reply.started": "2023-05-11T21:57:43.628480Z",
          "shell.execute_reply": "2023-05-11T21:57:43.635917Z"
        },
        "trusted": true,
        "id": "M_tDs1Dt1pA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### convert_examples_to_tf_dataset\n",
        "\n",
        "Esta es la parte más compleja del notebook. Vamos a transformar los Ejemplos de entrada anteriormente definidos en Features que entiende BERT. Se pueden ver 2 pasos fundamentales.\n",
        "\n",
        "1. Encoding con el tokenizador. Aquí se configuran la forma de los tokens de cada frase\n",
        "2. Cómo añade los tokens, la capa de atención y tipo de token dentro de un objeto y fuera la etiqueta de la frase.\n",
        "\n",
        "Esta manipulación que hemos hecho es lo que nos da el poder de clasificar nuestro dataset usando BERT."
      ],
      "metadata": {
        "id": "n-eo_hFrz_Dx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
        "    features = [] # -> InputFeatures\n",
        "\n",
        "    for e in tqdm(examples):\n",
        "        input_dict = tokenizer.encode_plus(\n",
        "            e.text_a,\n",
        "            add_special_tokens = True,    # añadimos 'CLS'  'SEP'\n",
        "            max_length=max_length,    # truncamos if len(s) > max_length\n",
        "            return_token_type_ids = True,\n",
        "            return_attention_mask = True,\n",
        "            pad_to_max_length = True, # pads to the right by default\n",
        "            truncation = True\n",
        "        )\n",
        "\n",
        "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
        "        features.append(InputFeatures( input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label) )\n",
        "\n",
        "    def gen():\n",
        "        for f in features:\n",
        "            yield (\n",
        "                {\n",
        "                    \"input_ids\": f.input_ids,\n",
        "                    \"attention_mask\": f.attention_mask,\n",
        "                    \"token_type_ids\": f.token_type_ids,\n",
        "                },\n",
        "                f.label,\n",
        "            )\n",
        "\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        gen,\n",
        "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
        "        (\n",
        "            {\n",
        "                \"input_ids\": tf.TensorShape([None]),\n",
        "                \"attention_mask\": tf.TensorShape([None]),\n",
        "                \"token_type_ids\": tf.TensorShape([None]),\n",
        "            },\n",
        "            tf.TensorShape([]),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "\n",
        "DATA_COLUMN = 'review'\n",
        "LABEL_COLUMN = 'sentiment'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-11T21:57:43.638486Z",
          "iopub.execute_input": "2023-05-11T21:57:43.639084Z",
          "iopub.status.idle": "2023-05-11T21:57:43.655400Z",
          "shell.execute_reply.started": "2023-05-11T21:57:43.639046Z",
          "shell.execute_reply": "2023-05-11T21:57:43.654383Z"
        },
        "trusted": true,
        "id": "syPl53bE1pA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
        "train_data = train_data.shuffle(100).batch(32).repeat(2) # Desordenamos los ejemplos y los guardamos en batch de 32 elementos"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-11T21:57:43.656822Z",
          "iopub.execute_input": "2023-05-11T21:57:43.657499Z",
          "iopub.status.idle": "2023-05-11T22:04:52.841122Z",
          "shell.execute_reply.started": "2023-05-11T21:57:43.657331Z",
          "shell.execute_reply": "2023-05-11T22:04:52.840109Z"
        },
        "trusted": true,
        "id": "XJ8_Opc01pA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n",
        "validation_data = validation_data.batch(32)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-11T22:04:52.842541Z",
          "iopub.execute_input": "2023-05-11T22:04:52.842941Z",
          "iopub.status.idle": "2023-05-11T22:05:40.392673Z",
          "shell.execute_reply.started": "2023-05-11T22:04:52.842901Z",
          "shell.execute_reply": "2023-05-11T22:05:40.391684Z"
        },
        "trusted": true,
        "id": "LeidjmNO1pA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listos para entrenar\n",
        "\n",
        "Ya hemos preparado el set de entrenamiento y validación, vamos a entrenar el modelo.\n",
        "\n",
        "**ATENCIÓN: Nos tomará 1 hora o más.**"
      ],
      "metadata": {
        "id": "3Z14X4Hg47Sv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
        "\n",
        "model.fit(train_data, epochs=2, validation_data=validation_data)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-11T22:05:40.401189Z",
          "iopub.execute_input": "2023-05-11T22:05:40.401701Z",
          "iopub.status.idle": "2023-05-11T22:52:13.010746Z",
          "shell.execute_reply.started": "2023-05-11T22:05:40.401660Z",
          "shell.execute_reply": "2023-05-11T22:52:13.010015Z"
        },
        "trusted": true,
        "id": "dUtM9NCD1pA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_sentences = ['Very boring movie, I would not recommend it to my friends', 'Wow, blew my mind, the animation and the story are amazing', 'Thoughtful and complex story but I enjoyed it very much']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-11T22:52:13.012949Z",
          "iopub.execute_input": "2023-05-11T22:52:13.013228Z",
          "iopub.status.idle": "2023-05-11T22:52:13.018793Z",
          "shell.execute_reply.started": "2023-05-11T22:52:13.013201Z",
          "shell.execute_reply": "2023-05-11T22:52:13.017871Z"
        },
        "trusted": true,
        "id": "vFm8Hw3T1pA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_batch = tokenizer(pred_sentences, max_length=128, padding=True, truncation=True, return_tensors='tf')   # Tokenizamos antes de enviar al modelo\n",
        "tf_outputs = model(tf_batch)\n",
        "\n",
        "# Softmax para la clasificación final porque el modelo devuelve otra cosa (logits) y luego el índice mayor es el que  nos ayuda a decidir si es positivo o negativo\n",
        "\n",
        "tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\n",
        "labels = ['Negative','Positive']\n",
        "label = tf.argmax(tf_predictions, axis=1)\n",
        "label = label.numpy()\n",
        "for i in range(len(pred_sentences)):\n",
        "    print(pred_sentences[i], \": \", labels[label[i]])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-11T22:52:13.020151Z",
          "iopub.execute_input": "2023-05-11T22:52:13.020531Z",
          "iopub.status.idle": "2023-05-11T22:52:13.116813Z",
          "shell.execute_reply.started": "2023-05-11T22:52:13.020489Z",
          "shell.execute_reply": "2023-05-11T22:52:13.115944Z"
        },
        "trusted": true,
        "id": "--KPJChM1pA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lecturas interesantes sobre BERT:\n",
        "1. https://towardsdatascience.com/sentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671\n",
        "2. https://medium.com/@dhartidhami/understanding-bert-word-embeddings-7dc4d2ea54ca"
      ],
      "metadata": {
        "trusted": true,
        "id": "LnozblQY1pA5"
      }
    }
  ]
}